import argparse
import os
import json
import json5
import time
import shutil
import logging
import re
from typing import List

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from dotenv import load_dotenv

import config
import specs
from infrastructure import MockDBManager
from rag_service import UnifiedRAGService
from agents import BusinessRuleAnalystAgent, TestCaseGeneratorAgent, TestStrategyPlannerAgent

load_dotenv()
logger = logging.getLogger("TA_Agent_Orchestrator")

# ==========================================
# Global Constants & Configuration
# ==========================================

# å»ºç«‹ Key åˆ°ä¸­æ–‡æè¿°çš„æ˜ å°„ï¼Œç”¨äºæç¤ºè¯å¢å¼º
# è¿™æ ·å¯ä»¥ä¿æŒ specs.py çš„çº¯å‡€ï¼ŒåŒæ—¶ç»™ Agent æä¾›è¯­ä¹‰ä¿¡æ¯
FILE_KEY_DESC_MAP = {
    "DIST_ACC": "é”€å”®å•†è´¦æˆ·ç”³è¯·æ–‡ä»¶",
    "DIST_TRADE": "é”€å”®å•†äº¤æ˜“ç”³è¯·æ–‡ä»¶",
    "MGR_NAV": "ç®¡ç†äººå‡€å€¼æ–‡ä»¶",
    "MGR_CONFIRM": "ç®¡ç†äººç¡®è®¤å›æ‰§æ–‡ä»¶"
}

# åŠ¨æ€ç”Ÿæˆ SUPPORTED_FILE_TYPESï¼Œç¡®ä¿ä¸ specs.py ä¸¥æ ¼åŒæ­¥
# æ ¼å¼ç¤ºä¾‹: ["DIST_TRADE - é”€å”®å•†äº¤æ˜“ç”³è¯·æ–‡ä»¶", ...]
SUPPORTED_FILE_TYPES = [
    f"{key} - {FILE_KEY_DESC_MAP.get(key, 'æœªå®šä¹‰æè¿°æ–‡ä»¶')}"
    for key in specs.FILE_SPECS.keys()
]

# ==========================================
# Main Orchestrator
# ==========================================
class Orchestrator:
    def __init__(self):
        self.db_manager = MockDBManager()
        # RAG æœåŠ¡ä»…ç”¨äºåˆå§‹åŒ–æ•°æ®å…¥åº“ï¼Œå…·ä½“çš„æŸ¥è¯¢ç”± Analyst Agent çš„ Tool æ¥ç®¡
        self.rag_service = UnifiedRAGService() 
        
        # åˆå§‹åŒ– Agents (å…¨éƒ¨æ¥è‡ª agents.py)
        self.planner = TestStrategyPlannerAgent(config.OPENAI_MODEL)
        self.analyst = BusinessRuleAnalystAgent()
        self.generator = TestCaseGeneratorAgent(config.OPENAI_MODEL)

    def initialize(self, reindex: bool = False):
        logger.info("Initializing Orchestrator...")
        if reindex:
            logger.info("Triggering Knowledge Base Ingestion...")
            self.rag_service.ingest_knowledge_base()
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(config.RULES_DIR, exist_ok=True)
        os.makedirs(config.DATA_DIR, exist_ok=True)
        logger.info("Initialization complete.")

    def phase_0_plan(self):
        """é˜¶æ®µé›¶ï¼šè‡ªåŠ¨è§„åˆ’æµ‹è¯•ä¸»é¢˜"""
        logger.info("ğŸš€ === PHASE 0: STRATEGY PLANNING ===")
        
        # ç›´æ¥ä½¿ç”¨å…¨å±€å˜é‡ï¼Œæ— éœ€å†…éƒ¨ import
        file_types_str = ", ".join([t.split(' - ')[0] for t in SUPPORTED_FILE_TYPES])
        
        topics = self.planner.plan(
            system_context=specs.SYSTEM_CONTEXT,
            file_interfaces=file_types_str
        )
        
        for i, t in enumerate(topics):
            logger.info(f"   {i+1}. {t}")
        return topics

    def phase_1_analyze(self, topics):
        """é˜¶æ®µä¸€ï¼šåˆ†ææ–‡æ¡£å’Œä»£ç ï¼Œæå–è§„åˆ™ (ä½¿ç”¨ Tool Calling Agent)"""
        logger.info("ğŸš€ === PHASE 1: AGENTIC ANALYSIS ===")
        
        for topic in topics:
            logger.info(f"ğŸ¤– Agent Analyzing Topic: {topic}")
            
            # Agent ä¼šè‡ªä¸»è°ƒç”¨ Tools (æŸ¥æ–‡æ¡£ã€æŸ¥ä»£ç ã€æŸ¥è§„èŒƒ)
            # æœ€ç»ˆè¿”å›è‡ªç„¶è¯­è¨€æˆ– JSON å­—ç¬¦ä¸²
            result_text = str(self.analyst.analyze(topic))
            
            # å°è¯•ä» Agent çš„å›å¤ä¸­æå– JSON éƒ¨åˆ†è¿›è¡Œæ¸…æ´—å’Œä¿å­˜
            try:
                cleaned_rules = self._extract_json_from_text(result_text)
            except Exception as e:
                logger.error(f"Error extracting JSON from Agent output: {e}")
                cleaned_rules = None
            
            if cleaned_rules:
                # æ–‡ä»¶åå®‰å…¨å¤„ç†
                safe_topic = "".join([c if c.isalnum() else '_' for c in topic])
                filename = f"rules_{int(time.time())}_{safe_topic[:50]}.json"
                filepath = os.path.join(config.RULES_DIR, filename)
                
                with open(filepath, 'w', encoding='utf-8') as f:
                    json.dump(cleaned_rules, f, indent=2, ensure_ascii=False)
                logger.info(f"âœ… Rules saved to: {filepath}")
            else:
                logger.warning(f"âš ï¸ Could not parse JSON from Agent output for topic: {topic}")
                # åŒæ—¶ä¹Ÿä¿å­˜åŸå§‹æ–‡æœ¬ä»¥ä¾¿ debug
                debug_path = os.path.join(config.RULES_DIR, f"debug_{int(time.time())}.txt")
                with open(debug_path, "w", encoding='utf-8') as f:
                    f.write(result_text)

    def phase_2_execute(self):
        """é˜¶æ®µäºŒï¼šç»“æ„åŒ–ç”Ÿæˆä¸å½’æ¡£ (ä½¿ç”¨ Structured Output Agent)"""
        logger.info("ğŸš€ === PHASE 2: STRUCTURED GENERATION ===")
        
        try:
            rule_files = [f for f in os.listdir(config.RULES_DIR) if f.endswith(".json")]
        except FileNotFoundError:
            logger.error(f"Rules directory not found.")
            return

        if not rule_files:
            logger.error(f"No rule files found. Run Phase 1 first.")
            return

        # åˆ›å»ºæ‰¹æ¬¡ç›®å½•
        batch_id = f"batch_{int(time.time())}"
        batch_dir = os.path.join(config.DATA_DIR, "generated_batches", batch_id)
        os.makedirs(batch_dir, exist_ok=True)
        logger.info(f"ğŸ“‚ Batch Directory: {batch_dir}")

        for r_file in rule_files:
            r_path = os.path.join(config.RULES_DIR, r_file)
            logger.info(f"ğŸ“‚ Processing Rules: {r_file}")
            
            try:
                with open(r_path, 'r', encoding='utf-8') as f:
                    rules = json5.load(f)
            except json5.JSONDecodeError:
                logger.error(f"Invalid JSON in {r_file}, skipping.")
                continue

            # å¦‚æœè§„åˆ™æ–‡ä»¶æ˜¯ Listï¼Œåˆ™éå†ï¼›å¦‚æœæ˜¯ Dictï¼Œåˆ™å°è£…
            if isinstance(rules, dict): rules = [rules]
            
            for rule in rules:
                rule_desc = rule.get('logic', str(rule)[:50])
                logger.info(f"âš¡ Generating Cases for: {rule_desc}...")
                
                # 1. ç¡®å®šè¾“å…¥æ–‡ä»¶ (è¿™é‡Œä¸ºäº†ç®€åŒ–ï¼Œä½¿ç”¨ç®€å•çš„å¯å‘å¼æˆ–å†æ¬¡è°ƒç”¨ LLMï¼Œ
                # ä½†ä¸ºäº†æ€§èƒ½ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ Generator Agent å†…éƒ¨å¤„ç†ï¼Œæˆ–è€…ç”± Analyst åœ¨ Phase 1 å·²ç»ç¡®å®š)
                # æ­¤å¤„æˆ‘ä»¬å°†æ‰€æœ‰ç›¸å…³ Context å–‚ç»™ Generator
                
                # ç¡®å®šç›¸å…³çš„æ–‡ä»¶è§„èŒƒ
                # ç®€å•ç­–ç•¥ï¼šæŠŠæ‰€æœ‰ Input å’Œ Output è§„èŒƒéƒ½å¡è¿›å»ï¼Œä¾é  LLM çš„æ³¨æ„åŠ›æœºåˆ¶
                full_spec_context = specs.GENERAL_SPECS + "\n"
                for key, content in specs.FILE_SPECS.items():
                    full_spec_context += f"\n--- INPUT SPEC: {key} ---\n{content}\n"
                for key, content in specs.OUTPUT_SPECS.items():
                    full_spec_context += f"\n--- OUTPUT SPEC: {key} ---\n{content}\n"

                # è°ƒç”¨ Pydantic å¼ºç±»å‹çš„ Generator Agent
                cases = self.generator.generate(
                    rule_json=json.dumps(rule, ensure_ascii=False),
                    interface_context=full_spec_context,
                    system_context=specs.SYSTEM_CONTEXT
                )
                
                for case in cases:
                    self._save_case_artifact(case, rule, r_file, batch_dir)

        logger.info(f"\nâœ… Generation Complete. Artifacts stored in {batch_dir}")

    def _save_case_artifact(self, case_dict, source_rule, source_file, batch_dir):
        """
        å°†å•ä¸ªæµ‹è¯•ç”¨ä¾‹çš„æ‰€æœ‰è¦ç´ ï¼ˆDBã€Inputã€Outputï¼‰ä¿å­˜ä¸ºç‹¬ç«‹çš„æ–‡ä»¶ç»“æ„ã€‚
        """
        case_id = case_dict.get('case_id', 'UNKNOWN_CASE')
        safe_case_id = "".join([c if c.isalnum() or c in ['_', '-'] else '_' for c in case_id])
        
        case_dir = os.path.join(batch_dir, safe_case_id)
        os.makedirs(case_dir, exist_ok=True)
        
        logger.info(f"      ğŸ’¾ Archiving Case: {case_id}")

        # 1. ä¿å­˜å…ƒæ•°æ® (meta.json)
        metadata = {
            "case_id": case_id,
            "description": case_dict.get('desc'),
            "source_rule": source_rule,
            "source_file": source_file,
            "expected_keyword": case_dict.get('expected_keyword'),
            "timestamp": int(time.time())
        }
        with open(os.path.join(case_dir, "meta.json"), 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)

        # 2. ä¿å­˜æ•°æ®åº“å¿«ç…§ (db_snapshot)
        snapshot_dir = os.path.join(case_dir, "db_snapshot")
        os.makedirs(snapshot_dir, exist_ok=True)
        setup_state = case_dict.get('setup_state', {})
        
        if 'accounts' in setup_state:
            with open(os.path.join(snapshot_dir, "Accounts.json"), 'w', encoding='utf-8') as f:
                json.dump(setup_state['accounts'], f, indent=2, ensure_ascii=False)
        if 'holdings' in setup_state:
            with open(os.path.join(snapshot_dir, "Holdings.json"), 'w', encoding='utf-8') as f:
                json.dump(setup_state['holdings'], f, indent=2, ensure_ascii=False)

        # 3. ä¿å­˜è¾“å…¥æ–‡ä»¶ (input_files)
        input_files_root = os.path.join(case_dir, "input_files")
        self._save_files(case_dict.get('input_files', []), input_files_root)

        # 4. ä¿å­˜é¢„æœŸè¾“å‡ºæ–‡ä»¶ (expected_output_files)
        output_files_root = os.path.join(case_dir, "expected_output_files")
        self._save_files(case_dict.get('output_files', []), output_files_root)

    def _save_files(self, file_list, root_dir):
        """è¾…åŠ©æ–¹æ³•ï¼šä¿å­˜æ–‡ä»¶åˆ—è¡¨ï¼ˆé€‚é… Pydantic dump åçš„ dict ç»“æ„ï¼‰"""
        if not file_list:
            return

        for file_obj in file_list:
            file_path = file_obj.get('path')
            file_content = file_obj.get('content')
            
            if file_path and file_content:
                clean_path = file_path.lstrip("/").lstrip("\\")
                if clean_path.startswith("./"): clean_path = clean_path[2:]
                
                full_path = os.path.join(root_dir, clean_path)
                os.makedirs(os.path.dirname(full_path), exist_ok=True)
                
                with open(full_path, 'w', encoding='utf-8') as f:
                    f.write(file_content)

    def _extract_json_from_text(self, text):
        """è¾…åŠ©æ–¹æ³•ï¼šä» Agent çš„è‡ªç„¶è¯­è¨€å›å¤ä¸­æå– JSON List"""
        try:
            return json5.loads(text)
        except json5.JSONDecodeError as e:
            print(e)
            pass
        
        match = re.search(r"```json(.*?)```", text, re.DOTALL)
        if match:
            try:
                return json5.loads(match.group(1))
            except json5.JSONDecodeError:
                pass
        
        match = re.search(r"(\[.*\])", text, re.DOTALL)
        if match:
            try:
                return json5.loads(match.group(1))
            except:
                pass
        return None

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="AI Agent for TA System Testing (Industrial Grade)")
    parser.add_argument("--step", choices=["analyze", "execute"], required=True, 
                        help="Choose 'analyze' (Phase 0+1) or 'execute' (Phase 2)")
    parser.add_argument("--reindex", action="store_true", help="Re-ingest knowledge base")
    parser.add_argument("--topics", nargs="+", help="Manually specify topics (overrides auto-planning)")
    
    args = parser.parse_args()
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    
    app = Orchestrator()
    app.initialize(reindex=args.reindex)

    if args.step == "analyze":
        # å¦‚æœæ²¡æœ‰æŒ‡å®š topicsï¼Œåˆ™è‡ªåŠ¨è§„åˆ’
        target_topics = args.topics if args.topics else app.phase_0_plan()
        app.phase_1_analyze(target_topics)
        
    elif args.step == "execute":
        app.phase_2_execute()