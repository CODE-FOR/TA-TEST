import argparse
import os
import json
import time
import shutil
import logging
import re
from typing import List

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from dotenv import load_dotenv

import config
import specs
from infrastructure import MockDBManager
from rag_service import UnifiedRAGService
# å¼•ç”¨æ–°é‡æ„çš„ Agents (æ”¯æŒ Tool Calling å’Œ Pydantic Schema)
from agents import BusinessRuleAnalystAgent, TestCaseGeneratorAgent

load_dotenv()
logger = logging.getLogger("TA_Agent_Orchestrator")

# ==========================================
# Phase 0: æˆ˜ç•¥è§„åˆ’ç»„ä»¶
# ==========================================
class TestStrategyPlanner:
    """
    [Phase 0] è´Ÿè´£ç”Ÿæˆæµ‹è¯•æˆ˜å½¹çš„å®è§‚ä¸»é¢˜ã€‚
    è¿™æ˜¯ä¸€ä¸ªè½»é‡çº§çš„ Chainï¼Œä¸éœ€è¦å¤æ‚çš„ Tool Callingï¼Œ
    åªéœ€åŸºäºç³»ç»Ÿä¸Šä¸‹æ–‡ (System Context) è¿›è¡Œå‘æ•£æ€§æ€è€ƒã€‚
    """
    def __init__(self, llm_model):
        self.llm = ChatOpenAI(model=llm_model, temperature=0.7) # é«˜æ¸©ä»¥æ¿€å‘çµæ„Ÿ

    def plan_test_campaign(self) -> List[str]:
        logger.info("ğŸ§  Brainstorming test scenarios based on System Specs...")
        
        template = """You are a Principal QA Architect for a Mission-Critical Financial System (Transfer Agent).
        Your goal is to design a comprehensive **Test Strategy** (List of Topics).

        ### 1. SYSTEM CONTEXT
        {system_context}

        ### 2. FILE INTERFACES
        The system handles: DIST_ACC, DIST_TRADE, MGR_NAV, MGR_CONFIRM.

        ### 3. STRATEGY GENERATION
        Generate 5-8 distinct, high-value **Test Topics**.
        Prioritize:
        - **Gap Analysis**: Discrepancies between Docs and potential Code reality.
        - **Boundary Attacks**: Max/min values, zero amounts.
        - **Process Interaction**: Account opening + Trading in same batch.
        
        Output strictly a JSON list of strings.
        Example: ["Redemption with insufficient balance", "Duplicate Account Opening"]
        """
        
        prompt = ChatPromptTemplate.from_template(template)
        chain = prompt | self.llm | JsonOutputParser()
        
        try:
            topics = chain.invoke({"system_context": specs.SYSTEM_CONTEXT})
            logger.info(f"ğŸ§  Planner generated {len(topics)} topics.")
            return topics
        except Exception as e:
            logger.error(f"Failed to plan strategy: {e}")
            return ["Redemption validation logic", "Account status checks"]

# ==========================================
# Main Orchestrator
# ==========================================
class Orchestrator:
    def __init__(self):
        self.db_manager = MockDBManager()
        # RAG æœåŠ¡ä»…ç”¨äºåˆå§‹åŒ–æ•°æ®å…¥åº“ï¼Œå…·ä½“çš„æŸ¥è¯¢ç”± Analyst Agent çš„ Tool æ¥ç®¡
        self.rag_service = UnifiedRAGService() 
        
        # åˆå§‹åŒ– Agents
        self.planner = TestStrategyPlanner(config.OPENAI_MODEL)
        self.analyst = BusinessRuleAnalystAgent()
        self.generator = TestCaseGeneratorAgent()

    def initialize(self, reindex: bool = False):
        logger.info("Initializing Orchestrator...")
        if reindex:
            logger.info("Triggering Knowledge Base Ingestion...")
            self.rag_service.ingest_knowledge_base()
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(config.RULES_DIR, exist_ok=True)
        os.makedirs(config.DATA_DIR, exist_ok=True)
        logger.info("Initialization complete.")

    def phase_0_plan(self):
        """é˜¶æ®µé›¶ï¼šè‡ªåŠ¨è§„åˆ’æµ‹è¯•ä¸»é¢˜"""
        logger.info("ğŸš€ === PHASE 0: STRATEGY PLANNING ===")
        topics = self.planner.plan_test_campaign()
        for i, t in enumerate(topics):
            logger.info(f"   {i+1}. {t}")
        return topics

    def phase_1_analyze(self, topics):
        """é˜¶æ®µä¸€ï¼šåˆ†ææ–‡æ¡£å’Œä»£ç ï¼Œæå–è§„åˆ™ (ä½¿ç”¨ Tool Calling Agent)"""
        logger.info("ğŸš€ === PHASE 1: AGENTIC ANALYSIS ===")
        
        for topic in topics:
            logger.info(f"ğŸ¤– Agent Analyzing Topic: {topic}")
            
            # Agent ä¼šè‡ªä¸»è°ƒç”¨ Tools (æŸ¥æ–‡æ¡£ã€æŸ¥ä»£ç ã€æŸ¥è§„èŒƒ)
            # æœ€ç»ˆè¿”å›è‡ªç„¶è¯­è¨€æˆ– JSON å­—ç¬¦ä¸²
            result_text = self.analyst.analyze(topic)
            
            # å°è¯•ä» Agent çš„å›å¤ä¸­æå– JSON éƒ¨åˆ†è¿›è¡Œæ¸…æ´—å’Œä¿å­˜
            cleaned_rules = self._extract_json_from_text(result_text)
            
            if cleaned_rules:
                # æ–‡ä»¶åå®‰å…¨å¤„ç†
                safe_topic = "".join([c if c.isalnum() else '_' for c in topic])
                filename = f"rules_{int(time.time())}_{safe_topic[:50]}.json"
                filepath = os.path.join(config.RULES_DIR, filename)
                
                with open(filepath, 'w', encoding='utf-8') as f:
                    json.dump(cleaned_rules, f, indent=2, ensure_ascii=False)
                logger.info(f"âœ… Rules saved to: {filepath}")
            else:
                logger.warning(f"âš ï¸ Could not parse JSON from Agent output for topic: {topic}")
                # åŒæ—¶ä¹Ÿä¿å­˜åŸå§‹æ–‡æœ¬ä»¥ä¾¿ debug
                debug_path = os.path.join(config.RULES_DIR, f"debug_{int(time.time())}.txt")
                with open(debug_path, "w", encoding='utf-8') as f:
                    f.write(result_text)

    def phase_2_execute(self):
        """é˜¶æ®µäºŒï¼šç»“æ„åŒ–ç”Ÿæˆä¸å½’æ¡£ (ä½¿ç”¨ Structured Output Agent)"""
        logger.info("ğŸš€ === PHASE 2: STRUCTURED GENERATION ===")
        
        try:
            rule_files = [f for f in os.listdir(config.RULES_DIR) if f.endswith(".json")]
        except FileNotFoundError:
            logger.error(f"Rules directory not found.")
            return

        if not rule_files:
            logger.error(f"No rule files found. Run Phase 1 first.")
            return

        # åˆ›å»ºæ‰¹æ¬¡ç›®å½•
        batch_id = f"batch_{int(time.time())}"
        batch_dir = os.path.join(config.DATA_DIR, "generated_batches", batch_id)
        os.makedirs(batch_dir, exist_ok=True)
        logger.info(f"ğŸ“‚ Batch Directory: {batch_dir}")

        for r_file in rule_files:
            r_path = os.path.join(config.RULES_DIR, r_file)
            logger.info(f"ğŸ“‚ Processing Rules: {r_file}")
            
            try:
                with open(r_path, 'r', encoding='utf-8') as f:
                    rules = json.load(f)
            except json.JSONDecodeError:
                logger.error(f"Invalid JSON in {r_file}, skipping.")
                continue

            # å¦‚æœè§„åˆ™æ–‡ä»¶æ˜¯ Listï¼Œåˆ™éå†ï¼›å¦‚æœæ˜¯ Dictï¼Œåˆ™å°è£…
            if isinstance(rules, dict): rules = [rules]
            
            for rule in rules:
                rule_desc = rule.get('logic', str(rule)[:50])
                logger.info(f"âš¡ Generating Cases for: {rule_desc}...")
                
                # è°ƒç”¨ Pydantic å¼ºç±»å‹çš„ Generator Agent
                # è¿™é‡Œå°† rule å¯¹è±¡è½¬ä¸ºå­—ç¬¦ä¸²ä¼ å…¥
                cases = self.generator.generate(json.dumps(rule, ensure_ascii=False))
                
                for case in cases:
                    self._save_case_artifact(case, rule, r_file, batch_dir)

        logger.info(f"\nâœ… Generation Complete. Artifacts stored in {batch_dir}")

    def _save_case_artifact(self, case_dict, source_rule, source_file, batch_dir):
        """
        [æ¢å¤çš„é«˜è´¨é‡ä¿å­˜é€»è¾‘]
        å°†å•ä¸ªæµ‹è¯•ç”¨ä¾‹çš„æ‰€æœ‰è¦ç´ ï¼ˆDBã€Inputã€Outputï¼‰ä¿å­˜ä¸ºç‹¬ç«‹çš„æ–‡ä»¶ç»“æ„ã€‚
        """
        case_id = case_dict.get('case_id', 'UNKNOWN_CASE')
        safe_case_id = "".join([c if c.isalnum() or c in ['_', '-'] else '_' for c in case_id])
        
        case_dir = os.path.join(batch_dir, safe_case_id)
        os.makedirs(case_dir, exist_ok=True)
        
        logger.info(f"      ğŸ’¾ Archiving Case: {case_id}")

        # 1. ä¿å­˜å…ƒæ•°æ® (meta.json)
        metadata = {
            "case_id": case_id,
            "description": case_dict.get('desc'),
            "source_rule": source_rule,
            "source_file": source_file,
            "expected_keyword": case_dict.get('expected_keyword'),
            "timestamp": int(time.time())
        }
        with open(os.path.join(case_dir, "meta.json"), 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)

        # 2. ä¿å­˜æ•°æ®åº“å¿«ç…§ (db_snapshot)
        snapshot_dir = os.path.join(case_dir, "db_snapshot")
        os.makedirs(snapshot_dir, exist_ok=True)
        setup_state = case_dict.get('setup_state', {})
        
        # æ‹†åˆ† Accounts å’Œ Holdings
        if 'accounts' in setup_state:
            with open(os.path.join(snapshot_dir, "Accounts.json"), 'w', encoding='utf-8') as f:
                json.dump(setup_state['accounts'], f, indent=2, ensure_ascii=False)
        if 'holdings' in setup_state:
            with open(os.path.join(snapshot_dir, "Holdings.json"), 'w', encoding='utf-8') as f:
                json.dump(setup_state['holdings'], f, indent=2, ensure_ascii=False)

        # 3. ä¿å­˜è¾“å…¥æ–‡ä»¶ (input_files)
        # æ³¨æ„ï¼šAgent ç°åœ¨è¿”å›çš„æ˜¯ List[FileArtifact]ï¼Œéœ€è¦é€‚é…
        input_files_root = os.path.join(case_dir, "input_files")
        self._save_files(case_dict.get('input_files', []), input_files_root)

        # 4. ä¿å­˜é¢„æœŸè¾“å‡ºæ–‡ä»¶ (expected_output_files)
        output_files_root = os.path.join(case_dir, "expected_output_files")
        self._save_files(case_dict.get('output_files', []), output_files_root)

    def _save_files(self, file_list, root_dir):
        """è¾…åŠ©æ–¹æ³•ï¼šä¿å­˜æ–‡ä»¶åˆ—è¡¨ï¼ˆé€‚é… Pydantic dump åçš„ dict ç»“æ„ï¼‰"""
        if not file_list:
            return

        for file_obj in file_list:
            # Pydantic dump åçš„å­—å…¸ key ä¸º 'path', 'content'
            file_path = file_obj.get('path')
            file_content = file_obj.get('content')
            
            if file_path and file_content:
                # è·¯å¾„æ¸…æ´—
                clean_path = file_path.lstrip("/").lstrip("\\")
                if clean_path.startswith("./"): clean_path = clean_path[2:]
                
                full_path = os.path.join(root_dir, clean_path)
                os.makedirs(os.path.dirname(full_path), exist_ok=True)
                
                with open(full_path, 'w', encoding='utf-8') as f:
                    f.write(file_content)

    def _extract_json_from_text(self, text):
        """è¾…åŠ©æ–¹æ³•ï¼šä» Agent çš„è‡ªç„¶è¯­è¨€å›å¤ä¸­æå– JSON List"""
        try:
            # å°è¯•ç›´æ¥è§£æ
            return json.loads(text)
        except json.JSONDecodeError:
            pass
        
        # å°è¯•æ­£åˆ™æå– ```json ... ``` å—
        match = re.search(r"```json(.*?)```", text, re.DOTALL)
        if match:
            try:
                return json.loads(match.group(1))
            except json.JSONDecodeError:
                pass
        
        # å°è¯•æå– [ ... ]
        match = re.search(r"(\[.*\])", text, re.DOTALL)
        if match:
            try:
                return json.loads(match.group(1))
            except:
                pass
                
        return None

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="AI Agent for TA System Testing (Industrial Grade)")
    parser.add_argument("--step", choices=["analyze", "execute"], required=True, 
                        help="Choose 'analyze' (Phase 0+1) or 'execute' (Phase 2)")
    parser.add_argument("--reindex", action="store_true", help="Re-ingest knowledge base")
    parser.add_argument("--topics", nargs="+", help="Manually specify topics (overrides auto-planning)")
    
    args = parser.parse_args()
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    
    app = Orchestrator()
    app.initialize(reindex=args.reindex)

    if args.step == "analyze":
        # å¦‚æœæ²¡æœ‰æŒ‡å®š topicsï¼Œåˆ™è‡ªåŠ¨è§„åˆ’
        target_topics = args.topics if args.topics else app.phase_0_plan()
        app.phase_1_analyze(target_topics)
        
    elif args.step == "execute":
        app.phase_2_execute()