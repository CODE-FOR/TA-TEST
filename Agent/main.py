import argparse
import os
import json
import time
import shutil
import logging
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser
import langchain
from dotenv import load_dotenv

import config
import specs
from infrastructure import MockDBManager
from rag_service import UnifiedRAGService
from java_connector import JavaSUTConnector
from agents import BusinessRuleAnalyst, TestCaseGenerator

load_dotenv()

# è®¾ç½® Logger
logger = logging.getLogger("TA_Agent_Orchestrator")

SUPPORTED_FILE_TYPES = [
    "DIST_ACC - é”€å”®å•†è´¦æˆ·ç”³è¯·æ–‡ä»¶",
    "DIST_TRADE - é”€å”®å•†äº¤æ˜“ç”³è¯·æ–‡ä»¶",
    "MGR_NAV - ç®¡ç†äººå‡€å€¼æ–‡ä»¶",
    "MGR_APPLY - ç®¡ç†äººå¾…ç¡®è®¤æ–‡ä»¶"
    "DIST_ACC_CONFIRM - é”€å”®å•†è´¦æˆ·ç¡®è®¤æ–‡ä»¶",
    "MGR_CONFIRM - ç®¡ç†äººç¡®è®¤æ–‡ä»¶",
    "DIST_CONFIRM - é”€å”®å•†äº¤æ˜“ç¡®è®¤æ–‡ä»¶"
]

class TestStrategyPlanner:
    """
    [Phase 0 Agent] æµ‹è¯•æˆ˜ç•¥è§„åˆ’å¸ˆ
    èŒè´£ï¼šç»¼åˆé˜…è¯»ç³»ç»Ÿæ–‡æ¡£ä¸æ ¸å¿ƒä»£ç é€»è¾‘ï¼Œåˆ©ç”¨å‘æ•£æ€ç»´ç”Ÿæˆå…¨é¢çš„æµ‹è¯•ä¸»é¢˜åˆ—è¡¨ã€‚
    """
    def __init__(self, llm, retriever):
        self.llm = llm
        self.retriever = retriever

    def plan_test_campaign(self) -> list[str]:
        logger.info("ğŸ§  Brainstorming test scenarios based on Specs AND Code Reality...")
        
        # 1. ä¸»åŠ¨æ£€ç´¢ä»£ç å±‚é¢çš„é€»è¾‘çº¿ç´¢
        code_context = ""
        try:
            # ä½¿ç”¨å®½æ³›ä½†é’ˆå¯¹é€»è¾‘çš„æŸ¥è¯¢è¯ï¼Œæ—¨åœ¨æå– Validate, Exception, Rule ç­‰æ ¸å¿ƒä»£ç 
            # è¿™æ · Agent å°±èƒ½çœ‹åˆ°æ–‡æ¡£æ²¡å†™çš„ç»†èŠ‚ï¼ˆä¾‹å¦‚ä»£ç é‡Œæ˜¯å¦æœ‰ VIP ç”¨æˆ·åˆ¤æ–­ï¼Ÿæ˜¯å¦æœ‰é‡‘é¢ä¸Šé™ï¼Ÿï¼‰
            search_query = "Business validation logic rules constraints exception handling"
            logger.info(f"   ğŸ” Scanning codebase for: '{search_query}'")
            
            docs = self.retriever.invoke(search_query)
            
            # æ ¼å¼åŒ–æ£€ç´¢åˆ°çš„ä»£ç ç‰‡æ®µ
            fragments = []
            for d in docs:
                source = d.metadata.get('source', 'unknown_file')
                content = d.page_content[:1000] # æˆªå–å‰1000å­—ç¬¦é¿å…Tokenæº¢å‡º
                fragments.append(f"--- Code Snippet from {source} ---\n{content}\n")
            
            code_context = "\n".join(fragments)
            logger.info(f"   -> Retrieved {len(docs)} code fragments to inform strategy.")
            
        except Exception as e:
            logger.warning(f"   âš ï¸ Failed to retrieve code context, planning based on docs only. Error: {e}")

        # 2. æˆ˜ç•¥è§„åˆ’ Prompt
        template = """You are a Principal QA Architect for a Mission-Critical Financial System (Transfer Agent).
        Your goal is to design a comprehensive **Test Strategy** (List of Topics) to uncover hidden bugs and discrepancies.

        ### 1. SYSTEM DOCUMENTATION (The Theory)
        {system_context}

        ### 2. CODE SIGNALS (The Reality)
        Below are snippets from the actual codebase. Use them to identify:
        - **Undocumented Logic**: Logic present in code but missing in docs (e.g., Hidden limits, VIP lists).
        - **Defensive Checks**: Specific `if` conditions or Exceptions thrown in code.
        - **Discrepancies**: Where Code implementation differs from Specs.
        
        Code Reality Context:
        {code_context}

        ### 3. INTERFACE SURFACE (The Attack Vectors)
        The system accepts these file types:
        {file_types}

        ### 4. STRATEGY GENERATION
        Combine Docs and Code insights to generate 5-10 distinct, high-value **Test Topics**.
        
        Prioritize:
        1. **Gap Analysis**: Topics that verify if code matches docs.
        2. **Boundary Attacks**: Based on actual thresholds found in code (or implied).
        3. **Process Interaction**: Complex state transitions.
        
        Example Output:
        ["Redemption > 1M manual check logic", "Duplicate Account Opening Attempt", "Nav file missing for T-day"]

        Output JSON List:
        """
        
        prompt = ChatPromptTemplate.from_template(template)
        chain = prompt | self.llm | JsonOutputParser()
        
        try:
            # æå–æ–‡ä»¶ç±»å‹çš„åç§°ä¾› Agent å‚è€ƒ
            file_type_names = [t.split(' - ')[0] for t in SUPPORTED_FILE_TYPES]
            
            topics = chain.invoke({
                "system_context": specs.SYSTEM_CONTEXT,
                "code_context": code_context,
                "file_types": ", ".join(file_type_names)
            })
            
            logger.info(f"ğŸ§  Strategy Planner generated {len(topics)} topics.")
            return topics
        except Exception as e:
            logger.error(f"Failed to plan test strategy: {e}", exc_info=True)
            # Fallback topics
            return ["Redeem insufficient shares check", "Duplicate account opening check"]

class Orchestrator:
    def __init__(self):
        # æé«˜æ¸©åº¦ä»¥å¢åŠ å‘æ•£æ€§
        self.llm = ChatOpenAI(model=config.OPENAI_MODEL, temperature=0.7) 
        self.db_manager = MockDBManager()
        self.rag = UnifiedRAGService()
        self.java_sut = JavaSUTConnector()
        
        # Agents - å°†åœ¨ initialize ä¸­å®ä¾‹åŒ–
        self.planner = None
        self.analyst = None
        self.generator = None

    def initialize(self, reindex: bool = False):
        logger.info("Initializing Orchestrator...")
        if reindex:
            logger.info("Triggering Knowledge Base Ingestion...")
            self.rag.ingest_knowledge_base()
        
        retriever = self.rag.get_retriever()
        
        # Phase 0: æˆ˜ç•¥è§„åˆ’ Agent (é«˜ Temperature, è¯»ä»£ç +æ–‡æ¡£)
        self.planner = TestStrategyPlanner(self.llm, retriever)
        
        # Phase 1 & 2: åˆ†æä¸ç”Ÿæˆ Agent (ä½ Temperature, ä¿è¯ä¸¥è°¨)
        precise_llm = ChatOpenAI(model=config.OPENAI_MODEL, temperature=0)
        self.analyst = BusinessRuleAnalyst(precise_llm, retriever)
        self.generator = TestCaseGenerator(precise_llm)
        
        os.makedirs(config.RULES_DIR, exist_ok=True)
        logger.info("Initialization complete. Agents ready.")

    def phase_0_plan(self):
        """é˜¶æ®µé›¶ï¼šè‡ªåŠ¨ç­–åˆ’æµ‹è¯•æˆ˜å½¹"""
        if not self.planner:
            logger.error("System not initialized.")
            return []

        logger.info("ğŸš€ === PHASE 0: STRATEGY PLANNING ===")
        topics = self.planner.plan_test_campaign()
        
        logger.info("ğŸ“‹ Generated Test Plan:")
        for i, t in enumerate(topics):
            logger.info(f"   {i+1}. {t}")
        
        return topics

    def phase_1_analyze(self, topics):
        """é˜¶æ®µä¸€ï¼šåˆ†ææ–‡æ¡£å’Œä»£ç ï¼Œæå–è§„åˆ™ä¾›äººå·¥å®¡æ ¸"""
        if not self.analyst:
            logger.error("System not initialized. Call initialize() first.")
            return

        logger.info("\nğŸš€ === PHASE 1: ANALYSIS & EXTRACTION ===")
        logger.info(f"Analyzing {len(topics)} topics...")

        for topic in topics:
            logger.info(f"ğŸ‘‰ Analyzing Topic: {topic}")
            try:
                rules = self.analyst.analyze(topic)
                
                # æ–‡ä»¶åå¢åŠ  safe å¤„ç†
                safe_topic = "".join([c if c.isalnum() else '_' for c in topic])
                filename = f"rules_{int(time.time())}_{safe_topic[:50]}.json"
                filepath = os.path.join(config.RULES_DIR, filename)
                
                with open(filepath, 'w', encoding='utf-8') as f:
                    json.dump(rules, f, indent=2, ensure_ascii=False)
                
                logger.info(f"âœ… Rules extracted to: {filepath}")
            except Exception as e:
                logger.error(f"âŒ Failed to analyze topic '{topic}': {e}", exc_info=True)

    def _identify_required_files(self, rule):
        logger.debug(f"Identifying required file types for rule: {rule.get('rule_id', 'UNKNOWN')}...")
        
        template = """You are a QA Architect.
        Identify required Interface Files for this rule based on V1.0 Spec.
        
        Rule: {rule}
        Available Types: {file_types}
        
        Return JSON list of keys (e.g., ["DIST_TRADE", "MGR_NAV"]).
        """
        prompt = ChatPromptTemplate.from_template(template)
        chain = prompt | self.llm | JsonOutputParser()
        try:
            available_keys = [t.split(' - ')[0] for t in SUPPORTED_FILE_TYPES]
            result_keys = chain.invoke({
                "rule": str(rule),
                "file_types": json.dumps(SUPPORTED_FILE_TYPES, ensure_ascii=False)
            })
            valid_keys = [k for k in result_keys if k in available_keys]
            
            logger.info(f"-> Agent identified input files: {valid_keys}")
            return valid_keys
        except Exception as e:
            logger.warning(f"Identification failed, falling back to default. Error: {e}")
            return ["DIST_TRADE"]

    def phase_2_execute(self):
        """é˜¶æ®µäºŒï¼šç”Ÿæˆæµ‹è¯•ç”¨ä¾‹å¹¶è¿›è¡Œç‹¬ç«‹å½’æ¡£å­˜å‚¨"""
        if not self.generator:
            logger.error("System not initialized.")
            return

        logger.info("\nğŸš€ === PHASE 2: GENERATION & ARTIFACT STORAGE (V1.0 Spec) ===")
        
        try:
            rule_files = [f for f in os.listdir(config.RULES_DIR) if f.endswith(".json")]
        except FileNotFoundError:
            logger.error(f"Rules directory not found: {config.RULES_DIR}")
            return

        if not rule_files:
            logger.error(f"No rule files found in {config.RULES_DIR}. Please run Phase 1 first.")
            return
        
        # åˆ›å»ºæœ¬æ¬¡è¿è¡Œçš„æ‰¹æ¬¡ç›®å½•
        batch_id = f"batch_{int(time.time())}"
        batch_dir = os.path.join(config.DATA_DIR, "generated_batches", batch_id)
        os.makedirs(batch_dir, exist_ok=True)
        logger.info(f"ğŸ“‚ Output Batch Directory created: {batch_dir}")

        for r_file in rule_files:
            r_path = os.path.join(config.RULES_DIR, r_file)
            logger.info(f"ğŸ“‚ Processing Rule File: {r_file}")
            
            try:
                with open(r_path, 'r', encoding='utf-8') as f:
                    rules = json.load(f)
            except json.JSONDecodeError as e:
                logger.error(f"Failed to load JSON from {r_file}: {e}")
                continue
            
            for rule in rules:
                rule_id = rule.get('rule_id', 'UNKNOWN')
                logger.info(f"âš¡ Generating Cases for Rule [{rule_id}]: {rule.get('logic', '')[:50]}...")
                
                # 1. ç¡®å®šè¾“å…¥æ–‡ä»¶
                target_file_keys = self._identify_required_files(rule)
                
                # 2. ç»„è£…ä¸Šä¸‹æ–‡
                full_context = specs.SYSTEM_CONTEXT + "\n" + specs.GENERAL_SPECS + "\n"
                for key in target_file_keys:
                    specific_spec = specs.FILE_SPECS.get(key)
                    if specific_spec:
                        full_context += f"\n--- SPEC FOR {key} ---\n{specific_spec}\n"
                full_context += "\n--- OUTPUT FILE REFERENCE ---\n" + str(specs.OUTPUT_SPECS)

                # 3. ç”Ÿæˆç”¨ä¾‹
                logger.debug(f"Invoking Generator Agent for Rule {rule_id}...")
                try:
                    cases = self.generator.generate(
                        rule, 
                        interface_context=full_context,
                        system_context=specs.SYSTEM_CONTEXT
                    )
                    logger.info(f"-> Generated {len(cases)} cases for Rule {rule_id}")
                except Exception as e:
                    logger.error(f"Failed to generate cases for Rule {rule_id}: {e}", exc_info=True)
                    continue
                
                # 4. ç‹¬ç«‹å½’æ¡£å­˜å‚¨æ¯ä¸ªç”¨ä¾‹
                for case in cases:
                    self._save_case_artifact(case, rule, r_file, batch_dir)

        logger.info(f"âœ… All test cases have been generated and archived in: {batch_dir}")

    def _save_case_artifact(self, case, rule, source_file, batch_dir):
        """
        å°†å•ä¸ªæµ‹è¯•ç”¨ä¾‹çš„æ‰€æœ‰è¦ç´ ï¼ˆDBã€Inputã€Outputï¼‰ä¿å­˜ä¸ºç‹¬ç«‹çš„æ–‡ä»¶ç»“æ„ã€‚
        """
        case_id = case.get('case_id', 'UNKNOWN_CASE')
        safe_case_id = "".join([c if c.isalnum() or c in ['_', '-'] else '_' for c in case_id])
        
        case_dir = os.path.join(batch_dir, safe_case_id)
        os.makedirs(case_dir, exist_ok=True)
        
        logger.info(f"      ğŸ’¾ Archiving Case: {case_id} -> {case_dir}")

        # 1. ä¿å­˜å…ƒæ•°æ® (meta.json)
        metadata = {
            "case_id": case_id,
            "description": case.get('desc'),
            "source_rule_id": rule.get('rule_id'),
            "source_rule_logic": rule.get('logic'),
            "source_rule_file": source_file,
            "expected_keyword": case.get('expected_keyword'),
            "timestamp": int(time.time())
        }
        with open(os.path.join(case_dir, "meta.json"), 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)

        # 2. ä¿å­˜æ•°æ®åº“å¿«ç…§ (db_snapshot)
        snapshot_dir = os.path.join(case_dir, "db_snapshot")
        os.makedirs(snapshot_dir, exist_ok=True)
        setup_state = case.get('setup_state', {})
        
        if 'accounts' in setup_state:
            with open(os.path.join(snapshot_dir, "Accounts.json"), 'w', encoding='utf-8') as f:
                json.dump(setup_state['accounts'], f, indent=2, ensure_ascii=False)
        if 'holdings' in setup_state:
            with open(os.path.join(snapshot_dir, "Holdings.json"), 'w', encoding='utf-8') as f:
                json.dump(setup_state['holdings'], f, indent=2, ensure_ascii=False)

        # 3. ä¿å­˜è¾“å…¥æ–‡ä»¶ (input_files)
        input_files_root = os.path.join(case_dir, "input_files")
        self._save_files(case.get('input_files', []), input_files_root, "input")

        # 4. ä¿å­˜é¢„æœŸè¾“å‡ºæ–‡ä»¶ (expected_output_files)
        output_files_root = os.path.join(case_dir, "expected_output_files")
        self._save_files(case.get('output_files', []), output_files_root, "output")

    def _save_files(self, file_list, root_dir, type_tag):
        """è¾…åŠ©æ–¹æ³•ï¼šä¿å­˜æ–‡ä»¶åˆ—è¡¨åˆ°æŒ‡å®šç›®å½•"""
        # å…¼å®¹æ—§æ ¼å¼ï¼ˆå•æ–‡ä»¶ï¼‰
        if not isinstance(file_list, list): 
            return 

        for file_obj in file_list:
            file_path = file_obj.get('path')
            file_content = file_obj.get('content')
            
            if file_path and file_content:
                clean_path = file_path.lstrip("/").lstrip("\\")
                if clean_path.startswith("./"):
                    clean_path = clean_path[2:]
                
                full_path = os.path.join(root_dir, clean_path)
                os.makedirs(os.path.dirname(full_path), exist_ok=True)
                
                with open(full_path, 'w', encoding='utf-8') as f:
                    f.write(file_content)
                logger.debug(f"        -> Saved {type_tag} file: {clean_path}")

    def execute_case(self, case):
        pass 

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--step", choices=["analyze", "execute"], required=True, 
                        help="Choose 'analyze' to generate rules, or 'execute' to generate test cases.")
    parser.add_argument("--reindex", action="store_true")
    parser.add_argument("--debug", action="store_true", help="Enable full LangChain debug logging")
    parser.add_argument("--topics", nargs="+", help="Manually specify topics (overrides auto-planning)")
    
    args = parser.parse_args()

    log_format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    
    if args.debug:
        print("ğŸ› Debug mode enabled: Monitoring all LLM inputs and outputs.")
        try:
            from langchain.globals import set_debug
            set_debug(True)
        except ImportError:
            langchain.debug = True
        logging.basicConfig(level=logging.DEBUG, format=log_format)
    else:
        logging.basicConfig(level=logging.INFO, format=log_format)

    app = Orchestrator()
    try:
        app.initialize(reindex=args.reindex)

        if args.step == "analyze":
            # é€»è¾‘å˜æ›´ï¼šå¦‚æœæœ‰æ‰‹åŠ¨ Topics åˆ™ä½¿ç”¨ï¼Œå¦åˆ™è¿›è¡Œè‡ªåŠ¨è§„åˆ’ (Phase 0)
            if args.topics:
                target_topics = args.topics
                logger.info(f"ğŸ“‹ Using Manual Topics: {target_topics}")
            else:
                target_topics = app.phase_0_plan()
            
            # è¿›å…¥ Phase 1
            app.phase_1_analyze(target_topics)
            
        elif args.step == "execute":
            app.phase_2_execute()
            
    except Exception as e:
        logger.critical(f"Unhandled exception in main execution: {e}", exc_info=True)